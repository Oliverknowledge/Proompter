ğŸ§  Proompter: Collaborative Prompt Evaluation & Optimization Platform
âš¡ Supercharge your AI workflows. Automate prompt evaluation. Build better promptsâ€”together.

ğŸš€ Problem
Prompt engineering is slow, subjective, and non-collaborative.
Teams often guess which prompt performs best, without scalable tools to measure, compare, or optimize.

ğŸ¯ Solution: Proompter
Proompter is a full-stack AI prompt evaluation & optimization platform that lets individuals and teams:

ğŸš€ Rapidly test, score, and compare prompts.

ğŸ§ª Run automated A/B tests with quantitative evaluation (relevance, hallucination, tone, etc.).

ğŸ¤ Collaborate in real-time with shared results, version history, and notifications.

Built with Next.js, Three.js, Python, Opik, LiteLLM, Supabase, and TailwindCSS, Proompter is designed to be hackathon-ready, dev-friendly, and scalable for production use.

ğŸ§© Key Features
Feature	Description
ğŸ§ª Automated Prompt Evaluation	Evaluate multiple prompts at once using Python + Opik + LiteLLM.
ğŸ“Š Multi-Metric Scoring	Assess prompts using relevance, hallucination, emotional tone, and CTA clarity.
ğŸ” Iterative Optimization	A/B test prompt variants and run multi-step optimization flows.
ğŸ‘¥ Team Collaboration	Share experiments, invite teammates, and track history together.
ğŸ“¦ Chunked Processing	Efficiently handles large prompt sets (both frontend and backend).
âœ¨ Modern UX	Beautiful UI with Framer Motion animations and Three.js visualizations.
ğŸ”§ Customizable	Drop in your own metrics or expand the system with your evaluation logic.

ğŸ› ï¸ Tech Stack
Layer	Stack
Frontend	Next.js Â· React Â· TailwindCSS Â· Framer Motion Â· Three.js
Backend	Next.js API Routes Â· Python (Opik + LiteLLM)
Auth & DB	Supabase (PostgreSQL + Auth + Realtime)
Infra & Hosting	Vercel (Frontend) Â· Supabase (Backend/Postgres)
AI Evaluation	LiteLLM wrapper for OpenAI & other LLMs Â· Opik for standardized prompt grading
Notifications	Telegram Bot Â· Email (via Supabase)

ğŸ§± Codebase Structure
ruby
Copy
Edit
proompter/
â”œâ”€â”€ app/                      # Next.js App Router
â”‚   â”œâ”€â”€ api/                 # API routes (e.g., /api/optimize)
â”‚   â””â”€â”€ dashboard/           # Authenticated dashboard views
â”œâ”€â”€ components/              # Reusable UI components (Tailwind + Framer Motion)
â”œâ”€â”€ lib/                     # Supabase client, auth utils, helpers
â”œâ”€â”€ scripts/                 # Python backend for evaluation
â”‚   â”œâ”€â”€ optimize.py          # Core evaluator
â”‚   â””â”€â”€ metrics/             # Pluggable custom metrics
â”œâ”€â”€ supabase/                # SQL migrations, schema, Supabase config
â”œâ”€â”€ public/                  # Static assets
â”œâ”€â”€ .env.example             # Example environment variables
â”œâ”€â”€ next.config.js           # Next.js config
â”œâ”€â”€ tailwind.config.js       # TailwindCSS config
â””â”€â”€ README.md                # This file
ğŸ“ˆ How It Works
Register & Authenticate: Secure signup via email or Google OAuth.

Submit Prompt Experiments: Input multiple prompts and task context.

Automated Evaluation: Prompts are sent to a Python backend where Opik + LiteLLM analyze them across multiple metrics.

Chunking Engine: Both frontend and backend automatically split large prompt batches.

Results & Optimization: Scores are returned and ranked; users can launch A/B tests or iterate.

Team Sharing: Collaborators can view, comment, and get notified about prompt experiments.

ğŸ”Œ Key API Endpoints
Endpoint	Method	Description
/api/optimize	POST	Evaluate a batch of prompts. Handles chunked requests, returns scores.
/api/optimization-runs	POST	Launch or continue a multi-step optimization session.
/api/run	POST	Run a single prompt test and get feedback.
auth/, teams/	Various	Handle authentication and team collaboration endpoints.

âš™ï¸ Local Setup
âœ… Requirements
Node.js 18+

Python 3.9+

Supabase project (setup or use provided migrations)

ğŸ§ª Quickstart
bash
Copy
Edit
git clone https://github.com/your-org/proompter.git
cd proompter
npm install
pip install -r requirements.txt
ğŸ” Environment Variables
Create .env from .env.example:

env
Copy
Edit
SUPABASE_URL=...
SUPABASE_ANON_KEY=...
OPENAI_API_KEY=...
TELEGRAM_BOT_TOKEN=...
ğŸš€ Run
bash
Copy
Edit
npm run dev  # launches frontend (Next.js)
Visit http://localhost:3000

ğŸ§© Extending Proompter
Task	How
Add new evaluation metric	Add a file in scripts/metrics/, then import it in optimize.py.
Add UI views	Use the Tailwind + Framer-powered components in components/ui/.
Add API logic	Add a new handler under app/api/.
Add team logic	Extend teams, prompt_history, optimization_runs in Supabase.

ğŸ‘¥ Team Collaboration
ğŸ‘¨â€ğŸ‘©â€ğŸ‘§ Invite teammates

ğŸ“Š Share and comment on experiments

ğŸ”” Get notified by email or Telegram when results are ready

ğŸ Hackathon Highlights
What We Solved	How
âŒ Prompt engineering is subjective	âœ… Used Opik to provide standardized scores for prompts
âŒ No scalable way to evaluate at once	âœ… Built chunking + batch API for massive prompt sets
âŒ No team-based prompt iteration tools	âœ… Added shared dashboards, history, and notifications
âŒ Hard to define â€œbestâ€ prompt	âœ… Built-in metrics + scoring engine + iterative optimizer

ğŸ“œ License
MIT License â€“ use freely, fork, and build upon it.

ğŸ’¬ Final Pitch
Proompter brings structure and scalability to the chaotic world of prompt engineering.
Instead of guessing, teams can now test, score, and optimize their prompts using robust AI metricsâ€”fast.

With an extensible architecture, a real-time collaborative UI, and powerful evaluation logic, Proompter is built to empower the next generation of AI creators.

Would you like a deployment badge, demo link section, or images/gifs added? I can tailor it further for judges with visuals or a walkthrough section.
